---
title: 2025JS
date: 2025-06-09
category:
  - Python
---



### 2.4-请求

```python
import requests

url = 'https://www.eastmoney.com/'


response = requests.get(url)
status = response.status_code

# 需要和下面写入文件的编码格式一样，不然会乱码
response.encoding = 'utf-8'
print(status)


if status == 200:
    page_text = response.text
    with open('test.html', 'w', encoding='utf-8') as f:
        f.write(page_text)
```

### 2.5-请求参数

```python
import requests

url = 'https://game.51.com/search/action/game/'

game_title = input('请输入一个游戏名称：')
params = {'q':game_title}
response = requests.get(url,params)
page_text = response.text

file_name = game_title + '.html'
with open(file_name, 'w', encoding='utf-8') as f:
    f.write(page_text)
```

### 2.5-反爬例子

```python
import requests

url = 'http://www.cpta.com.cn/'
response = requests.get(url)
page_text = response.text
with open('kaoshi.html', 'w', encoding='utf-8') as f:
    f.write(page_text)
```

被拦截：

![image-20250609154416604](http://www.iocaop.com/images/2025-06/20250609154416749.png)

加上UA:

```python
import requests

url = 'http://www.cpta.com.cn/'
headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'}
response = requests.get(url, headers=headers)
page_text = response.text
with open('kaoshi.html', 'w', encoding='utf-8') as f:
    f.write(page_text)
```

可以正常获取数据。

![image-20250609155626936](http://www.iocaop.com/images/2025-06/20250609155627018.png)

### 2.7-post

```python
import requests

url = 'http://www.cpta.com.cn/category/search'
headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'}
data = {
"keywords":"财务管理",
" 搜 索":"搜 索"
}
response = requests.post(url, headers=headers,data=data)
page_text = response.text
with open('财务管理.html', 'w', encoding='utf-8') as f:
    f.write(page_text)
```

### 2.8-动态加载数据

```python
import requests
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
    # 需加上，此站有防盗链
    'referer': 'https://www.icve.com.cn/portal_new/course/course.html?keyvalue=%E6%9C%BA%E6%A2%B0'
}
data = {
    "kczy": "",
    "order": "",
    "printstate": "",
    "keyvalue": "机械"
}

for page in range(1, 11):
    if page == 1:
        url = 'https://www.icve.com.cn/portal/course/getNewCourseInfo'
    else:
        # 格式化字符串
        url = 'https://www.icve.com.cn/portal/course/getNewCourseInfo?page=%d' % page
    response = requests.post(url, headers=headers, data=data)
    # 用response.json()将数据转成Python对象类型dict
    json_data = response.json()
    course_list = json_data['list']
    for course in course_list:
        print(course['TeacherDisplayname'])
```

### 2.9-图片爬取

```python
import requests
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
}

img_url = 'http://gips2.baidu.com/it/u=1674525583,3037683813&fm=3028&app=3028&f=JPEG&fmt=auto?w=1024&h=1024'
response = requests.get(img_url, headers=headers)

# 二进制数据获取，对标Java的字节流
c = response.content
with open('image.jpg', 'wb') as f:
    f.write(c)
```

### 3.2-xpath

安装：

```python
pip install lxml
```

示例html:

```html
<html lang="en">
<head>
	<meta charset="UTF-8" />
	<title>测试bs4</title>
</head>
<body>
	<div>
		<p>百里守约</p>
	</div>
	<div class="song">
		<p>李清照</p>
		<p>王安石</p>
		<p>苏轼</p>
		<p>柳宗元</p>
		<a href="http://www.song.com/" title="赵匡胤" target="_self">
			<span>this is span</span>
		宋朝是最强大的王朝，不是军队的强大，而是经济很强大，国民都很有钱</a>
		<a href="" class="du">总为浮云能蔽日,长安不见使人愁</a>
		<img src="http://www.baidu.com/meinv.jpg" alt="" />
	</div>
	<div class="tang">
		<ul>
			<li><a href="http://www.baidu.com" title="qing">清明时节雨纷纷,路上行人欲断魂,借问酒家何处有,牧童遥指杏花村</a></li>
			<li><a href="http://www.163.com" title="qin">秦时明月汉时关,万里长征人未还,但使龙城飞将在,不教胡马度阴山</a></li>
			<li><a href="http://www.126.com" alt="qi">岐王宅里寻常见,崔九堂前几度闻,正是江南好风景,落花时节又逢君</a></li>
			<li><a href="http://www.sina.com" class="du">杜甫</a></li>
			<li><a href="http://www.dudu.com" class="du">杜牧</a></li>
			<li><b>杜小月</b></li>
			<li><i>度蜜月</i></li>
			<li><a href="http://www.haha.com" id="feng">凤凰台上凤凰游,凤去台空江自流,吴宫花草埋幽径,晋代衣冠成古丘</a></li>
		</ul>
	</div>
</body>
</html>
```

```python
from lxml import etree

# 1.创建etree对象，将html加载
tree = etree.parse('day03-3.2.html')
# 2.调用xpath函数，结合表达式提取数据

# 写法1：全局匹配
ret = tree.xpath('//title')
# 返回结果ret是一个列表
print(ret)
# 写法2 路径匹配
ret = tree.xpath('/html/head/title')
print(ret[0])

# 会定位到满足要求的所有标签
div_ret = tree.xpath('//div')
print(div_ret)

# 根据属性定位
div_class_ret = tree.xpath('//div[@class="song"]')
print(div_class_ret)

# 索引定位：所以 从1开始
index_ret = tree.xpath('//div[1]')
print(index_ret)

# 写法举例：全局匹配满足层级关系
demo1 = tree.xpath('//div/ul/li/a')
print(demo1)

# /表示一层  // 表示1层或多层
demo2 = tree.xpath('//ul//a')
print(demo2)

#取出文本
demo3 = tree.xpath('//div/a[@class="du"]/text()')
print(demo3)
```

### 3.3-小说《碧血剑》爬取

*  url：https://bixuejian.5000yan.com/ 

- 需求：将每一个章节的标题和内容进行爬取然后存储到文件中'

```python
import os

import requests
from lxml import etree

main_url = 'https://bixuejian.5000yan.com/'
response = requests.get(main_url)
response.encoding = 'utf-8'
page_text = response.text
# print(page_text)

tree = etree.HTML(response.text)
a_list = tree.xpath('//ul[@class="mx-auto  row row-cols-1 row-cols-sm-2 row-cols-lg-3"]//a')

os.makedirs('./xiaoshuo_lib', exist_ok=True)
for a in a_list:
    a_href = a.xpath('@href')[0]
    a_text = a.xpath('text()')[0]
    # print(a_text,a_href)
    res = requests.get(a_href)
    res.encoding = 'utf-8'
    tree = etree.HTML(res.text)
    p_list = tree.xpath('//div[@class="grap"]//text()')
    content = ''.join(p_list)
    with open(f'./xiaoshuo_lib/{a_text}.txt', 'w', encoding='utf-8') as f:
        f.write(content)

```

谷歌浏览器可以生成表达式：

![image-20250609184614669](http://www.iocaop.com/images/2025-06/20250609184614741.png)

### 3.4-简历爬取

* 简历模版下载：https://sc.chinaz.com/jianli/free.html
* 下载当前页所有的简历模板

```python
import os

import requests
from lxml import etree

os.makedirs('./jianli_lib', exist_ok=True)
url = 'https://sc.chinaz.com/jianli/free.html'
res = requests.get(url)
res.encoding = 'utf-8'
# print(res.text)
tree = etree.HTML(res.text)
a_list = tree.xpath('//div[@class="box col3 ws_block"]/p/a')
for a in a_list:
    a_href = a.xpath('@href')[0]
    a_text = a.xpath('text()')[0]
    # print(a_href, a_text)
    a_res = requests.get(a_href)
    a_res.encoding = 'utf-8'
    a_tree = etree.HTML(a_res.text)
    a_down = a_tree.xpath('//*[@id="down"]/div[2]/ul/li[3]/a')
    for a in a_down:
        a_down_href = a.xpath('@href')[0]
        a_down_text = a.xpath('text()')[0]
        print(a_down_href, a_down_text)
        resp = requests.get(a_down_href)
        with open("jianli_lib/"+a_text+".rar","wb") as f:
            f.write(resp.content)

```

### 3.5-图片懒加载爬取

* url：https://sc.chinaz.com/tupian/meinvtupian.html

```python
import os

import requests
from lxml import etree

os.makedirs('./img_lib', exist_ok=True)
url = 'https://sc.chinaz.com/tupian/meinvtupian_3.html'
req = requests.get(url)
req.encoding = 'utf-8'
# print(req.text)
tree = etree.HTML(req.text)
img_list = tree.xpath('//div[@class="item"]/img')
for img in img_list:
    img_name = img.xpath('@alt')[0]
    img_url =('http:'+ img.xpath('@data-original')[0]).replace('_s.','.')
    print(img_name,img_url)
    resp = requests.get(img_url)
    with open('./img_lib/'+img_name+'.jpg','wb') as f:
        f.write(resp.content)
```

- 如何实现图片懒加载/动态加载？
  - 使用img标签的伪属性（指的是自定义的一种属性）。在网页中，为了防止图片马上加载出来，则在img标签中可以使用一种伪属性来存储图片的链接，而不是使用真正的src属性值来存储图片链接。（图片链接一旦给了src属性，则图片会被立即加载出来）。只有当图片被滑动到浏览器可视化区域范围的时候，在通过js将img的伪属性修改为真正的src属性，则图片就会被加载出来。
- 如何爬取图片懒加载的图片数据？
  - 只需要在解析图片的时候，定位伪属性的属性值即可

![image-20250610135854170](http://www.iocaop.com/images/2025-06/20250610135854943.png)

### 4.1-防盗链

- 现在很多网站启用了防盗链反爬，防止服务器上的资源被人恶意盗取。什么是防盗链呢？

  -  从HTTP协议说起，在HTTP协议中，有一个表头字段：referer，采用URL的格式来表示从哪一个链接跳转到当前网页的。通俗理解就是：客户端的请求具体从哪里来，服务器可以通过referer进行溯源。一旦检测来源不是网页所规定的，立即进行阻止或者返回指定的页面。

- 案例：抓取微博图片，url：http://blog.sina.com.cn/lm/pic/，将页面中某一组系列详情页的图片进行抓取保存，比如三里屯时尚女郎：http://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1

  - 注意：

    - 1.在解析图片地址的时候，定位src的属性值，返回的内容和开发工具Element中看到的不一样，通过network查看网页源码发现需要解析real_src的值。

    - 2.直接请求real_src请求到的图片不显示，加上Refere请求头即可
      - 哪里找Refere：抓包工具定位到某一张图片数据包，在其requests headers中获取

```python
import os

import requests
from lxml import etree

os.makedirs('./img_lib', exist_ok=True)
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36',
    'Referer':'https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1'
}
url = 'https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1'
res = requests.get(url)
tree = etree.HTML(res.text)
img_list = tree.xpath('//*[@id="sina_keyword_ad_area2"]/div/a/img')
i=0
for img in img_list:
    img_href = img.xpath('@real_src')[0]
    response = requests.get(img_href,headers=headers)
    i=i+1
    with open('./img_lib/'+str(i)+'.jpg', 'wb') as f:
        f.write(response.content)
```

### 4.2-代理

隧道代理：隧道代理IP是一种动态代理技术，用户通过固定的代理服务器地址发起请求，服务器会自动将请求转发到不同的代理IP上，实现IP的频繁更换。其核心在于云端自动管理IP切换，无需人工干预。

- 代理的匿名度

  - 透明：网站的服务器知道你使用了代理，也知道你的真实ip
  - 匿名：网站服务器知道你使用了代理，但是无法获知你真实的ip
  - 高匿：网站服务器不知道你使用了代理，也不知道你的真实ip（推荐）

- 代理的类型（重要）

  - http：该类型的代理服务器只可以转发http协议的请求
  - https：可以转发https协议的请求  

- 如何获取代理?

  - 携趣代理：https://www.xiequ.cn/index.html?f301de7f

测试：访问如下网址，返回自己本机ip：

```python
import requests
from lxml import etree
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36',
}
url = 'http://www.cip.cc/'

page_text = requests.get(url,headers=headers).text
tree = etree.HTML(page_text)
text = tree.xpath('/html/body/div/div/div[3]/pre/text()')[0]
print(text.split('\n')[1])
```

使用代理发起请求，查看是否可以返回代理服务器的ip:

```python
import requests
from lxml import etree
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36',
}
url = 'http://www.cip.cc/'

page_text = requests.get(url,headers=headers,proxies={'http':'182.38.125.232:3828'}).text
tree = etree.HTML(page_text)
text = tree.xpath('/html/body/div/div/div[3]/pre/text()')[0]
print(text.split('\n')[1])
```

### 4.3-代理池构建

测试网站：https://wz.sun0769.com/political/index/politicsNewest?id=1

多次频繁请求：

```python
import requests
from lxml import etree

for page in range(1,100):
    url = 'https://wz.sun0769.com/political/index/politicsNewest?id=1&page=%d' % page
    response = requests.get(url)
    tree = etree.HTML(response.content)
    text = tree.xpath('/html/body/div[2]/div[3]/ul[2]/li[1]/span[3]/a/text()')[0]
    print(text)
```

![image-20250610151644390](http://www.iocaop.com/images/2025-06/20250610151644519.png)

![image-20250610151710554](http://www.iocaop.com/images/2025-06/20250610151710648.png)

下面用代理进行：

![image-20250610151837234](http://www.iocaop.com/images/2025-06/20250610151837314.png)

在代码中获取ip池中的ip：

```python
import random

import  requests
from lxml import etree


# 一次性获取多个IP
def get_proxy_ip_port():
    url = 'http://api.xiequ.cn/VAD/GetIp.aspx?act=get&uid=161726&vkey=B8FC6DAB4158AC3EA48ADCEF09137350&num=5&time=30&plat=0&re=0&type=2&so=1&ow=1&spl=1&addr=&db=1'
    response = requests.get(url)
    ip_data = response.json()
    return ip_data['data']

ip_list = get_proxy_ip_port()
for page in range(1,100):
    # 取随机ip进行访问
    ip_info = random.choice(ip_list)
    ip, port = ip_info['IP'], ip_info['Port']
    url = 'https://wz.sun0769.com/political/index/politicsNewest?id=1&page=%d' % page
    response = requests.get(url,proxies={'https':'%s:%s'%(ip,port)})
    response.encoding ='utf-8'
    tree = etree.HTML(response.text)
    # print(response.text)
    text = tree.xpath('/html/body/div[2]/div[3]/ul[2]/li[1]/span[3]/a/text()')[0]
    print(text)
```

### 4.4-中大网校考试中心题目爬取

需求：https://ks.wangxiao.cn/，每个一级目录下有二级目录，每个二级目录详情页下会有【每日一练】，把所有一级目录下对应所有二级目录的【每日一练】的所有题目爬下来。

