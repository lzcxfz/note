---
title: 2025JS
date: 2025-06-09
category:
  - Python
---



### 2.4-请求

```python
import requests

url = 'https://www.eastmoney.com/'


response = requests.get(url)
status = response.status_code

# 需要和下面写入文件的编码格式一样，不然会乱码
response.encoding = 'utf-8'
print(status)


if status == 200:
    page_text = response.text
    with open('test.html', 'w', encoding='utf-8') as f:
        f.write(page_text)
```

### 2.5-请求参数

```python
import requests

url = 'https://game.51.com/search/action/game/'

game_title = input('请输入一个游戏名称：')
params = {'q':game_title}
response = requests.get(url,params)
page_text = response.text

file_name = game_title + '.html'
with open(file_name, 'w', encoding='utf-8') as f:
    f.write(page_text)
```

### 2.5-反爬例子

```python
import requests

url = 'http://www.cpta.com.cn/'
response = requests.get(url)
page_text = response.text
with open('kaoshi.html', 'w', encoding='utf-8') as f:
    f.write(page_text)
```

被拦截：

![image-20250609154416604](http://www.iocaop.com/images/2025-06/20250609154416749.png)

加上UA:

```python
import requests

url = 'http://www.cpta.com.cn/'
headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'}
response = requests.get(url, headers=headers)
page_text = response.text
with open('kaoshi.html', 'w', encoding='utf-8') as f:
    f.write(page_text)
```

可以正常获取数据。

![image-20250609155626936](http://www.iocaop.com/images/2025-06/20250609155627018.png)

### 2.7-post

```python
import requests

url = 'http://www.cpta.com.cn/category/search'
headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'}
data = {
"keywords":"财务管理",
" 搜 索":"搜 索"
}
response = requests.post(url, headers=headers,data=data)
page_text = response.text
with open('财务管理.html', 'w', encoding='utf-8') as f:
    f.write(page_text)
```

### 2.8-动态加载数据

```python
import requests
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
    # 需加上，此站有防盗链
    'referer': 'https://www.icve.com.cn/portal_new/course/course.html?keyvalue=%E6%9C%BA%E6%A2%B0'
}
data = {
    "kczy": "",
    "order": "",
    "printstate": "",
    "keyvalue": "机械"
}

for page in range(1, 11):
    if page == 1:
        url = 'https://www.icve.com.cn/portal/course/getNewCourseInfo'
    else:
        # 格式化字符串
        url = 'https://www.icve.com.cn/portal/course/getNewCourseInfo?page=%d' % page
    response = requests.post(url, headers=headers, data=data)
    # 用response.json()将数据转成Python对象类型dict
    json_data = response.json()
    course_list = json_data['list']
    for course in course_list:
        print(course['TeacherDisplayname'])
```

### 2.9-图片爬取

```python
import requests
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
}

img_url = 'http://gips2.baidu.com/it/u=1674525583,3037683813&fm=3028&app=3028&f=JPEG&fmt=auto?w=1024&h=1024'
response = requests.get(img_url, headers=headers)

# 二进制数据获取，对标Java的字节流
c = response.content
with open('image.jpg', 'wb') as f:
    f.write(c)
```

### 3.2-xpath

安装：

```python
pip install lxml
```

示例html:

```html
<html lang="en">
<head>
	<meta charset="UTF-8" />
	<title>测试bs4</title>
</head>
<body>
	<div>
		<p>百里守约</p>
	</div>
	<div class="song">
		<p>李清照</p>
		<p>王安石</p>
		<p>苏轼</p>
		<p>柳宗元</p>
		<a href="http://www.song.com/" title="赵匡胤" target="_self">
			<span>this is span</span>
		宋朝是最强大的王朝，不是军队的强大，而是经济很强大，国民都很有钱</a>
		<a href="" class="du">总为浮云能蔽日,长安不见使人愁</a>
		<img src="http://www.baidu.com/meinv.jpg" alt="" />
	</div>
	<div class="tang">
		<ul>
			<li><a href="http://www.baidu.com" title="qing">清明时节雨纷纷,路上行人欲断魂,借问酒家何处有,牧童遥指杏花村</a></li>
			<li><a href="http://www.163.com" title="qin">秦时明月汉时关,万里长征人未还,但使龙城飞将在,不教胡马度阴山</a></li>
			<li><a href="http://www.126.com" alt="qi">岐王宅里寻常见,崔九堂前几度闻,正是江南好风景,落花时节又逢君</a></li>
			<li><a href="http://www.sina.com" class="du">杜甫</a></li>
			<li><a href="http://www.dudu.com" class="du">杜牧</a></li>
			<li><b>杜小月</b></li>
			<li><i>度蜜月</i></li>
			<li><a href="http://www.haha.com" id="feng">凤凰台上凤凰游,凤去台空江自流,吴宫花草埋幽径,晋代衣冠成古丘</a></li>
		</ul>
	</div>
</body>
</html>
```

```python
from lxml import etree

# 1.创建etree对象，将html加载
tree = etree.parse('day03-3.2.html')
# 2.调用xpath函数，结合表达式提取数据

# 写法1：全局匹配
ret = tree.xpath('//title')
# 返回结果ret是一个列表
print(ret)
# 写法2 路径匹配
ret = tree.xpath('/html/head/title')
print(ret[0])

# 会定位到满足要求的所有标签
div_ret = tree.xpath('//div')
print(div_ret)

# 根据属性定位
div_class_ret = tree.xpath('//div[@class="song"]')
print(div_class_ret)

# 索引定位：所以 从1开始
index_ret = tree.xpath('//div[1]')
print(index_ret)

# 写法举例：全局匹配满足层级关系
demo1 = tree.xpath('//div/ul/li/a')
print(demo1)

# /表示一层  // 表示1层或多层
demo2 = tree.xpath('//ul//a')
print(demo2)

#取出文本
demo3 = tree.xpath('//div/a[@class="du"]/text()')
print(demo3)
```

### 3.3-小说《碧血剑》爬取

*  url：https://bixuejian.5000yan.com/ 

- 需求：将每一个章节的标题和内容进行爬取然后存储到文件中'

```python
import os

import requests
from lxml import etree

main_url = 'https://bixuejian.5000yan.com/'
response = requests.get(main_url)
response.encoding = 'utf-8'
page_text = response.text
# print(page_text)

tree = etree.HTML(response.text)
a_list = tree.xpath('//ul[@class="mx-auto  row row-cols-1 row-cols-sm-2 row-cols-lg-3"]//a')

os.makedirs('./xiaoshuo_lib', exist_ok=True)
for a in a_list:
    a_href = a.xpath('@href')[0]
    a_text = a.xpath('text()')[0]
    # print(a_text,a_href)
    res = requests.get(a_href)
    res.encoding = 'utf-8'
    tree = etree.HTML(res.text)
    p_list = tree.xpath('//div[@class="grap"]//text()')
    content = ''.join(p_list)
    with open(f'./xiaoshuo_lib/{a_text}.txt', 'w', encoding='utf-8') as f:
        f.write(content)

```

谷歌浏览器可以生成表达式：

![image-20250609184614669](http://www.iocaop.com/images/2025-06/20250609184614741.png)

### 3.4-简历爬取

* 简历模版下载：https://sc.chinaz.com/jianli/free.html
* 下载当前页所有的简历模板

```python
import os

import requests
from lxml import etree

os.makedirs('./jianli_lib', exist_ok=True)
url = 'https://sc.chinaz.com/jianli/free.html'
res = requests.get(url)
res.encoding = 'utf-8'
# print(res.text)
tree = etree.HTML(res.text)
a_list = tree.xpath('//div[@class="box col3 ws_block"]/p/a')
for a in a_list:
    a_href = a.xpath('@href')[0]
    a_text = a.xpath('text()')[0]
    # print(a_href, a_text)
    a_res = requests.get(a_href)
    a_res.encoding = 'utf-8'
    a_tree = etree.HTML(a_res.text)
    a_down = a_tree.xpath('//*[@id="down"]/div[2]/ul/li[3]/a')
    for a in a_down:
        a_down_href = a.xpath('@href')[0]
        a_down_text = a.xpath('text()')[0]
        print(a_down_href, a_down_text)
        resp = requests.get(a_down_href)
        with open("jianli_lib/"+a_text+".rar","wb") as f:
            f.write(resp.content)

```

### 3.5-图片懒加载爬取

* url：https://sc.chinaz.com/tupian/meinvtupian.html

```python
import os

import requests
from lxml import etree

os.makedirs('./img_lib', exist_ok=True)
url = 'https://sc.chinaz.com/tupian/meinvtupian_3.html'
req = requests.get(url)
req.encoding = 'utf-8'
# print(req.text)
tree = etree.HTML(req.text)
img_list = tree.xpath('//div[@class="item"]/img')
for img in img_list:
    img_name = img.xpath('@alt')[0]
    img_url =('http:'+ img.xpath('@data-original')[0]).replace('_s.','.')
    print(img_name,img_url)
    resp = requests.get(img_url)
    with open('./img_lib/'+img_name+'.jpg','wb') as f:
        f.write(resp.content)
```

- 如何实现图片懒加载/动态加载？
  - 使用img标签的伪属性（指的是自定义的一种属性）。在网页中，为了防止图片马上加载出来，则在img标签中可以使用一种伪属性来存储图片的链接，而不是使用真正的src属性值来存储图片链接。（图片链接一旦给了src属性，则图片会被立即加载出来）。只有当图片被滑动到浏览器可视化区域范围的时候，在通过js将img的伪属性修改为真正的src属性，则图片就会被加载出来。
- 如何爬取图片懒加载的图片数据？
  - 只需要在解析图片的时候，定位伪属性的属性值即可

![image-20250610135854170](http://www.iocaop.com/images/2025-06/20250610135854943.png)

### 4.1-防盗链

- 现在很多网站启用了防盗链反爬，防止服务器上的资源被人恶意盗取。什么是防盗链呢？

  -  从HTTP协议说起，在HTTP协议中，有一个表头字段：referer，采用URL的格式来表示从哪一个链接跳转到当前网页的。通俗理解就是：客户端的请求具体从哪里来，服务器可以通过referer进行溯源。一旦检测来源不是网页所规定的，立即进行阻止或者返回指定的页面。

- 案例：抓取微博图片，url：http://blog.sina.com.cn/lm/pic/，将页面中某一组系列详情页的图片进行抓取保存，比如三里屯时尚女郎：http://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1

  - 注意：

    - 1.在解析图片地址的时候，定位src的属性值，返回的内容和开发工具Element中看到的不一样，通过network查看网页源码发现需要解析real_src的值。

    - 2.直接请求real_src请求到的图片不显示，加上Refere请求头即可
      - 哪里找Refere：抓包工具定位到某一张图片数据包，在其requests headers中获取

```python
import os

import requests
from lxml import etree

os.makedirs('./img_lib', exist_ok=True)
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36',
    'Referer':'https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1'
}
url = 'https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1'
res = requests.get(url)
tree = etree.HTML(res.text)
img_list = tree.xpath('//*[@id="sina_keyword_ad_area2"]/div/a/img')
i=0
for img in img_list:
    img_href = img.xpath('@real_src')[0]
    response = requests.get(img_href,headers=headers)
    i=i+1
    with open('./img_lib/'+str(i)+'.jpg', 'wb') as f:
        f.write(response.content)
```

### 4.2-代理

隧道代理：隧道代理IP是一种动态代理技术，用户通过固定的代理服务器地址发起请求，服务器会自动将请求转发到不同的代理IP上，实现IP的频繁更换。其核心在于云端自动管理IP切换，无需人工干预。

- 代理的匿名度

  - 透明：网站的服务器知道你使用了代理，也知道你的真实ip
  - 匿名：网站服务器知道你使用了代理，但是无法获知你真实的ip
  - 高匿：网站服务器不知道你使用了代理，也不知道你的真实ip（推荐）

- 代理的类型（重要）

  - http：该类型的代理服务器只可以转发http协议的请求
  - https：可以转发https协议的请求  

- 如何获取代理?

  - 携趣代理：https://www.xiequ.cn/index.html?f301de7f

测试：访问如下网址，返回自己本机ip：

```python
import requests
from lxml import etree
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36',
}
url = 'http://www.cip.cc/'

page_text = requests.get(url,headers=headers).text
tree = etree.HTML(page_text)
text = tree.xpath('/html/body/div/div/div[3]/pre/text()')[0]
print(text.split('\n')[1])
```

使用代理发起请求，查看是否可以返回代理服务器的ip:

```python
import requests
from lxml import etree
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36',
}
url = 'http://www.cip.cc/'

page_text = requests.get(url,headers=headers,proxies={'http':'182.38.125.232:3828'}).text
tree = etree.HTML(page_text)
text = tree.xpath('/html/body/div/div/div[3]/pre/text()')[0]
print(text.split('\n')[1])
```

### 4.3-代理池构建

测试网站：https://wz.sun0769.com/political/index/politicsNewest?id=1

多次频繁请求：

```python
import requests
from lxml import etree

for page in range(1,100):
    url = 'https://wz.sun0769.com/political/index/politicsNewest?id=1&page=%d' % page
    response = requests.get(url)
    tree = etree.HTML(response.content)
    text = tree.xpath('/html/body/div[2]/div[3]/ul[2]/li[1]/span[3]/a/text()')[0]
    print(text)
```

![image-20250610151644390](http://www.iocaop.com/images/2025-06/20250610151644519.png)

![image-20250610151710554](http://www.iocaop.com/images/2025-06/20250610151710648.png)

下面用代理进行：

![image-20250610151837234](http://www.iocaop.com/images/2025-06/20250610151837314.png)

在代码中获取ip池中的ip：

```python
import random

import  requests
from lxml import etree


# 一次性获取多个IP
def get_proxy_ip_port():
    url = 'http://api.xiequ.cn/VAD/GetIp.aspx?act=get&uid=161726&vkey=B8FC6DAB4158AC3EA48ADCEF09137350&num=5&time=30&plat=0&re=0&type=2&so=1&ow=1&spl=1&addr=&db=1'
    response = requests.get(url)
    ip_data = response.json()
    return ip_data['data']

ip_list = get_proxy_ip_port()
for page in range(1,100):
    # 取随机ip进行访问
    ip_info = random.choice(ip_list)
    ip, port = ip_info['IP'], ip_info['Port']
    url = 'https://wz.sun0769.com/political/index/politicsNewest?id=1&page=%d' % page
    response = requests.get(url,proxies={'https':'%s:%s'%(ip,port)})
    response.encoding ='utf-8'
    tree = etree.HTML(response.text)
    # print(response.text)
    text = tree.xpath('/html/body/div[2]/div[3]/ul[2]/li[1]/span[3]/a/text()')[0]
    print(text)
```

### 4.4-中大网校考试中心题目爬取

需求：https://ks.wangxiao.cn/，每个一级目录下有二级目录，每个二级目录详情页下会有【每日一练】，把所有一级目录下对应所有二级目录的【每日一练】的所有题目爬下来。

```python
import io
import json
import os

import requests
from docx.oxml.ns import qn
from docx.shared import Pt, RGBColor, Inches
from lxml import etree
from docx import Document
from bs4 import BeautifulSoup

headers = {
    'Accept': 'application/json, text/javascript, */*; q=0.01',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Content-Type': 'application/json; charset=UTF-8',
    'Origin': 'https://ks.wangxiao.cn',
    'Referer': 'https://ks.wangxiao.cn/practice/getQuestion?practiceType=1&sign=jzs1&subsign=5166078fbf1eed222fe9&day=20250609',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'sec-ch-ua': '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'Cookie': 'acw_tc=3adad01817495610953552347e93a9eaad03204d586f8faff396001397; wxLoginUrl=http%3A%2F%2Fks.wangxiao.cn%2Fpractice%2FgetQuestion%3FpracticeType%3D1%26sign%3Djzs1%26subsign%3D5166078fbf1eed222fe9%26day%3D20250610; safedog-flow-item=; UserCookieName=ztk_12394272; OldUsername2=h%2Bj9hmYRjBu%2BNMB1TLMNDw%3D%3D; OldUsername=h%2Bj9hmYRjBu%2BNMB1TLMNDw%3D%3D; OldPassword=RMaC%2BoiWmXw%3D; UserCookieName_=ztk_12394272; OldUsername2_=h%2Bj9hmYRjBu%2BNMB1TLMNDw%3D%3D; OldUsername_=h%2Bj9hmYRjBu%2BNMB1TLMNDw%3D%3D; OldPassword_=RMaC%2BoiWmXw%3D; ztk_12394272_exam=ky; mantis6894=09946c45310b4bcd8bbdf45f09c394c3@6894; acw_sc__v3=68482fbfba6c2eaa7ec6c59b67409cce0b7df1a0; tfstk=glQmnDi-9i-jmIyJ2aTbYicLkvrJlET6KO39BFpa4LJWkqCZkFjGLs5YGECwIGvPYi3vBFTG_9C93FIvHClfr90tk-Zf_VY97JeLJyCbGFTap0ax7cGX6Ckq3AKZ4Y89sezeLnCfGFGSwArd4sGGlLNeuFWwz3RBOFuq0O8r4LO6_xRZu7VkFCmZ7d8wzbRH9d8w7O5zZC9yQF-VQ_logVJh7a_zeNoJ5lAWAaADmp5uAVui1IkpKsrsSV7lqnvESL0Z7aAVsTSQb2q9LM9vvpWzyqYGaCXve9zErU-hA1dGnr02kgS5lEQ8KXtcIKKFr3Dq0BYDnH72VX0hMhSlrEQ4CD1RZK-HygE7HhLcnM9B0uw5I_vAQZxrn8pOv_Q2q9y-rOtGA1dGnr0VLg5S4DzRMVOz6aosfnRWZpLzwiQk2HCe77VoAot2NIeLZ7msfnRWZpFuZDM60QOYp; userInfo=%7B%22userName%22%3A%22ztk_12394272%22%2C%22token%22%3A%2274566d2c-5edb-42a2-be4b-cbc9ff6fe86e%22%2C%22headImg%22%3A%22https%3A%2F%2Fthirdwx.qlogo.cn%2Fmmopen%2Fvi_32%2FDYAIOgq83eptPhP5Yak0v9w51Ypk4owO47sH0yus1IgBoicZD6go9XEnpkWxU0slofnnbhyRzW4SdBJQHA6IJqQ%2F132%22%2C%22nickName%22%3A%22176****5285%22%2C%22sign%22%3A%22ky%22%2C%22isBindingMobile%22%3A%221%22%2C%22isSubPa%22%3A%220%22%2C%22userNameCookies%22%3A%22h%2Bj9hmYRjBu%2BNMB1TLMNDw%3D%3D%22%2C%22passwordCookies%22%3A%22RMaC%2BoiWmXw%3D%22%7D; token=74566d2c-5edb-42a2-be4b-cbc9ff6fe86e'
}

# 获取一级目录
def get_category_level1():
    url = 'https://ks.wangxiao.cn/'
    res = requests.get(url)
    res.encoding = 'utf-8'
    tree = etree.HTML(res.text)
    # print(res.text)
    element_list = tree.xpath('//ul[@class="first-title"]/li')
    return element_list


# 获取二级目录dict_list  name,sign
def get_category_level2(category_level1):
    element_list = category_level1.xpath('.//div[@class="send-title"]/a')
    res_dict = list(map(lambda x: {'name': x.xpath('text()')[0], 'sign': x.xpath('@href')[0].split('=')[-1]}, element_list))
    return res_dict


# 获取每日一练dict_list
def get_everyday_homework(level2_dict):
    name = level2_dict['name']
    url = 'https://ks.wangxiao.cn/practice/listEveryday?sign=' + level2_dict['sign']
    res = requests.get(url)
    res.encoding = 'utf-8'
    # print(res.text)
    tree = etree.HTML(res.text)
    test_list = tree.xpath('//ul[@class="test-item"]')
    res_dict = list(map(lambda x: {'name': x.xpath('.//li/text()')[1],
                                   'subsign':x.xpath('.//li/a/@href')[0].split('&')[2].split('=')[-1],
                                   'day':x.xpath('.//li/a/@href')[0].split('&')[3].split('=')[-1]
                                   },test_list))
    return res_dict

# 获取每日一练详情(题目)
def get_everyday_homework_question(sign,subsign,day):
   url = 'https://ks.wangxiao.cn/practice/listQuestions'
   payload = {"practiceType":"1","sign":sign,"subsign":subsign,"day":day}
   response = requests.request("POST", url, headers=headers, data=json.dumps(payload))
   response.encoding= 'utf-8'
   json_data = response.json()
   return json_data

# 题目、选项样式
def set_question_style(paragraph):
    for run in paragraph.runs:
        run.font.name = '宋体'
        run._element.rPr.rFonts.set(qn('w:eastAsia'), '宋体')
        run.font.size = Pt(12)
        run.font.color.rgb = RGBColor(0, 0, 0)

# 解析样式
def set_analysis_style(paragraph):
    for run in paragraph.runs:
        run.font.name = '宋体'
        run._element.rPr.rFonts.set(qn('w:eastAsia'), '宋体')
        run.font.size = Pt(8)
        run.font.color.rgb = RGBColor(255, 0, 0)

# 持久化每日一练内容
def save_question_to_word(data,homework_dict):
    document = Document()
    doc_name = homework_dict['name']+homework_dict['day']
    document.add_heading(doc_name, 0)
    paper_type_code = data['paperRule']['sort']
    print(data['paperRule']['content'],data['paperRule']['sort'])
    if paper_type_code != 1:
        return
    document.add_heading(data['paperRule']['content'], 2)
    question_list = data['questions']
    # 题目
    i = 1
    for question in question_list:
        # print(question['content'])
        soup = BeautifulSoup(question['content'], 'html.parser')
        clean_text = soup.get_text()
        paragraph =document.add_paragraph(str(i)+'.'+clean_text)
        set_question_style(paragraph)
        option_list = question['options']
        # 选项
        for option in option_list:
            soup = BeautifulSoup(option['content'], 'html.parser')
            clean_text = soup.get_text()
            option_text = option['name']+'.'+clean_text
            if option['isRight']==1:
                option_text+='✔️'
            paragraph = document.add_paragraph(option_text)
            set_question_style(paragraph)
        # 解析
        soup = BeautifulSoup(question['textAnalysis'],'html.parser')
        clean_text = soup.get_text()
        paragraph =document.add_paragraph('解析：'+clean_text)
        set_analysis_style(paragraph)
        if question['textAnalysis']:
            tree = etree.HTML(question['textAnalysis'])
            img_src_list = tree.xpath('//img/@src')
            if len(img_src_list)>0:
                img_src = img_src_list[0]
                if not img_src:
                    continue
                elif img_src.startswith('http://') or img_src.startswith('https://'):
                    pass
                elif img_src.startswith('//'):
                    img_src = 'http:' + img_src
                else:
                    img_src = 'http://' + img_src
                response = requests.get(img_src,headers=headers)
                if response.status_code != 200:
                    print('图片响应码',response.status_code)
                    continue
                image_data = io.BytesIO(response.content)
                document.add_picture(image_data,width=Inches(3))
        i = i+1
    document.save('./homework_lib/'+doc_name+'.docx')


# 批量
def batch_down():
    category_level1_list = get_category_level1()
    for category_level1 in category_level1_list:
        level2_dict_list = get_category_level2(category_level1)
        for level2_dict in level2_dict_list:
            # 只爬公务员题目
            if level2_dict['sign'] == 'gwy':
                homework_dict_list = get_everyday_homework(level2_dict)
                for homework_dict in homework_dict_list:
                   question_dict =  get_everyday_homework_question(level2_dict['sign'],homework_dict['subsign'],homework_dict['day'])
                   # print(question_dict)
                   save_question_to_word(question_dict['Data'][0], homework_dict)

if __name__ == '__main__':
    os.makedirs('./homework_lib', exist_ok=True)
    batch_down()
    # level1 = get_category_level1()
    # level2_dict_list = get_category_level2(level1[0])
    # a = get_everyday_homework(level2_dict_list[0])
    # # print(a)
    # b= get_everyday_homework_question(level2_dict_list[0]['sign'], a[0]['subsign'],
    #                                a[0]['day'])
    # print(b['Data'][0])
    # save_question_to_word(b['Data'][0],a[0])
    # print(level1)
    # for item in level1:
    #     level1_name = item.xpath('.//span/text()')[0]
    #     # print(level1_name)
    #     # for dict in level2_dict_list:
    #         # print(dict)

```

